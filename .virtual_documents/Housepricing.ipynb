import sys
assert sys.version_info>=(3,5)

import sklearn
assert sklearn.__version__>="0.20"

import numpy as np
import os #for os operations

# to plot pretty graphs

import matplotlib as mpl
# displays plots within the cell
%matplotlib inline
import matplotlib.pyplot as plt
# setting up graph parameters
mpl.rc('axes',labelsize=14)
mpl.rc('xtick',labelsize=12)
mpl.rc('ytick',labelsize=12)

#setting up project dir to save images and files
# "." dot indicates to current folder
PROJECT_ROOT_DIR="."
CHAPTER_ID ="end_to_end_project"
#joining paths 
IMAGE_PATH=os.path.join(PROJECT_ROOT_DIR,"images",CHAPTER_ID)
os.makedirs(IMAGE_PATH,exist_ok=True)

# function to save plotted graphs in high res and as png 
def save_fig(fig_id,tight_layout=True,fig_extension="png",resolution=300):
    path=os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)


# getting/fetching the data from url

import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"
def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    #extracting csv file from tarfile 
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()



fetch_housing_data()
#fetching data 



import pandas as pd
#loading csv file into a df using oandas
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)


housing=load_housing_data()
housing.head()


housing.info()


housing['ocean_proximity'].value_counts()


housing.describe()


#generating hsitogram for each numerical column
housing.hist(bins=50,figsize=(20,15))

save_fig("attribute_histogram_plots")
plt.show()








np.random.seed(42)


def split_train_test(data,test_ratio):
    #len(data) returns number of rows
    #np.random.perm.. returns an array of random indices from 0 to len(data)-1
    shuffled_indices=np.random.permutation(len(data))
    test_set_size=int(len(data)*test_ratio)
    test_indices=shuffled_indices[:test_set_size]
    train_indices=shuffled_indices[test_set_size:]
    return data.iloc[train_indices],data.iloc[test_indices]    


train_set,test_set=split_train_test(housing,0.2)
len(train_set)


len(test_set)


#the above method used is traditional method but contains flaws : 
# it will fail when the new data will be added
# better method is using hashmap using crc32 algo
# but for the hash map thing we need an identifier and we dont have an identifier column hence simplest method is using row index


from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

# there are seveeral other methods too for splitting the data using hashing


# splitting using functions of sklearn 
# features provided : - the functions works same as the random seed generator one 
# - additional features it provides is, it can split two datasets by taking them as parameter
#   (with identical number of rows) and it will split it using the same indexes
#   (example we have different dataframe for labels)
# using this method is fine if our data set is very huge but fails when the dataset is small like in our case,
# using this method can cause sampling bias
# suppose a survey company decides to call 1000 people to ask quesitions it wont call randomly , suppose us population 
# got 52 % male and 48 % female then the sample of 1000 must be the representative of the whole popultion hence the
# selected 1000 people will contain 480 females and 520 males this is called stratified sampling : 
# the population is divided into homogenous subgroups called strata and the right number of instances is sampled
# from each stratum ( homogenous subgroups ) is selected to ensure the test set is representative of 
# whole data set
# if any attribute has continuous numerical values then it will form too many stratas ( homogenous subgrouos ) hence we categorize this data 
# and now the stratas will be less and the split of train and test data is easier and now the test data can be made representative of the whole
# data by using stratified shuffling



housing['income_cat']=pd.cut(housing["median_income"],bins=[0.,1.5,3.0,4.5,6,np.inf],labels=[1,2,3,4,5])


housing['income_cat'].value_counts()


housing['income_cat'].hist()
plt.show()


from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)
for train_index,test_index in split.split(housing,housing["income_cat"]):
    strat_train_set=housing.loc[train_index]
    strat_test_set=housing.loc[test_index]
    # splitting accorrding to income category ensuring the ratio is maintained.


# lets see if the proportions we wanted to maintain were actually maintained or not

strat_test_set['income_cat'].value_counts()/len(strat_test_set)





#checking proportions in parent dataset - housing
housing['income_cat'].value_counts()/len(housing)





#Now you should remove the income_cat attribute so the data is back to its original state:

for set_ in (strat_train_set, strat_test_set): 
    set_.drop("income_cat", axis=1, inplace=True)


housing.head()




test_set.head()
# housing['median_house_value'].value_counts()


strat_test_set.head()
# we will continue with the strat sets only











# creating copy
housing_copy=housing.copy()
housing=strat_train_set.copy()


# visualizing the geaographical data
housing.plot(kind="scatter", x="longitude", y="latitude")
plt.show()


# setting up alpha=0.1 helps us to see where the datapoints are densely present the
# region appears brighter than the others
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
plt.show()


housing.plot(
    kind="scatter",
    x="longitude",
    y="latitude",
    alpha=0.4,
    s=housing["population"] / 100,
    label="Population",
    figsize=(10, 7),
    c="median_house_value",
    cmap=plt.get_cmap("jet"),
    colorbar=True,
)

plt.legend()
plt.show()
save_fig("housing_prices_scatterplot")





# LOOKING FOR CORRELATIONS 
# Since the dataset is not too large, you can easily compute the 
# standard correlation coefficient (also called Pearson’s r) between every 
# pair of attributes using the corr() method:

corr_matrix=housing.corr(numeric_only=True)
corr_matrix['median_house_value'].sort_values(ascending=True)





from pandas.plotting import scatter_matrix
attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age" ]
scatter_matrix(housing[attributes],figsize=(12,8))
plt.show()


# after analyzing the correlation matrix it is evident that the most promising attribut to predict the 
# median house value is median income so lets analyze and zoom in on their correlation
housing.plot(kind="scatter",x="median_income",y="median_house_value",alpha=0.1)
plt.show()





housing.plot(
    kind="scatter", 
    x="median_income", 
    y="median_house_value", 
    alpha=0.4,
    s=housing["population"] / 100,  # bubble size
    label="population",
    figsize=(10, 7),
    c="median_house_value",  # color by price
    cmap=plt.get_cmap("jet"), 
    colorbar=True,
)
plt.legend()
plt.show()








housing.head()


housing['rooms_per_household']=housing['total_rooms']/housing['households']
housing['population_per_household']=housing['population']/housing["households"]
housing['bedrooms_per_room']=housing['total_bedrooms']/housing['total_rooms']



corr_matrix=housing.corr(numeric_only=True)
corr_matrix['median_house_value'].sort_values(ascending=False)








housing=strat_train_set.drop("median_house_value",axis=1)
housing_label=strat_train_set['median_house_value'].copy()
housing.info()





# as we know moat learning algos cant  work with missing values hence we need to handle them in some way :
# three methods :
# 1. get rid of districts - dropna
# 2. get rid of whole attribute - drop function
# 3. set the values to some value (zero,median ir the mean etc) using fillna



# if we use the option 3 then we neeed to dave the value somewhere as we need to replace the missing values in test set as well
# and same goes when working with the new or updated data.
# another method is using sklearn's imputer classs
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")
# since the median can be computed only for the numerical values then we will create a copy of train seet with all numerical values
# skipping ocean proximity attribute
# this is applied on the all columns hence the missing values of all the columns will 
# be replaced by their median values this is the reason we need to remove
# non numeriical columns
housing_num=housing.drop("ocean_proximity",axis=1)
imputer.fit(housing_num)
# the above token trains the imputer , it calculates and stores the median value of each column.
# these medians will later be used to fill in missing values.
# initially imputer saves them in .statistics_



imputer.statistics_



#checking if its same as manually computing the values
housing_num.median().values


# this actually replaces the missing values with the median and returns a numpy array as all the outputs of sklearn gives numpy array as outputs as the calcualtions using numpys are faster

x= imputer.transform(housing_num)
x


housing_tr=pd.DataFrame(x,columns=housing_num.columns)
housing_tr








housing_cat=housing[['ocean_proximity']]
housing_cat.head()


# most ml alf=gorithms prefer to work with numbers so lets convert this to numerical by encoding:
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder=OrdinalEncoder()
housing_cat_encoded=ordinal_encoder.fit_transform(housing_cat)
# the above token returns an nd array and then it is stored in housing cat encoded 
housing_cat_encoded[:10]
# type(housing_cat_encoded)
#Problem:
# ML models treat numbers like ordered values.
# So it might assume:

# INLAND (1) is closer to ISLAND (2) than to NEAR OCEAN (4) — which makes no sense here!


# solution to above problem is one hot encoding
from sklearn.preprocessing import OneHotEncoder
cat_encoder=OneHotEncoder()
housing_cat_1hot=cat_encoder.fit_transform(housing_cat)
housing_cat_1hot

# the output here is sparse matrix which contains zero and 1 for he presence of the attributes like studied befoe
# this one hot encoding forms new attributes and these are called dummy attributes if one attribute is present
# suppose one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise) 
# sparse matrix only stores the location and value of non zero attribute as shown below
# [
#  (0, 2)  1.0,
#  (1, 1)  1.0,
#  (2, 3)  1.0,
#  ...
# ] sparse matrix


#converting to array
housing_cat_1hot.toarray()



cat_encoder.categories_


# all the built in functions of sklearn we used for transformation are called transformer but we can also 
# make our own custom transformers
# we use these transformers mainly in data preprocessing pipelines
# and what all we have done with the data till now before training the mdoel is part of the data preprocessing pipeline











# from sklearn import BaseEstimator, TransformerMixin
# rooms_ix,bedrooms_ix,population_ix,households_ix = 3,4,5,6

# class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
#     def __init__(self,add_bedrooms_per_room=True):
#         sef.add_bedrooms_per_room = add_bedrooms_per_room
#     def fit(self,X,y=None):
#         return self
#     def transform(self,X,y=None):
#         rooms_per_household=X[:,rooms_ix]/X[:,households_ix]
#         population_per_household=X[:,population_ix]/X[:,households_ix]
#         if self.add_bedrooms_per_room :
#             bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]
#             return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]
#         else:
#             return np.c_[X,rooms_per_household,population_per_household]
# attr_adder=Combined








# - Machine learning algos dont perform well when the input numerical attribute have very different scales.
# - the algo becomes biased towards the higher scale values like in our case it will be no. of rooms 
#   which is less important than the median income.
# - to tackle this we use feature scaling 

# TIP :
# - As with all esitmators we only fit the scalers to training data only.
# - we never use fit( ) or fit.transform for any other set than the training set by doing this we will be letting the model peek into the test or validation set.
# - once we have trained the scaler on the training set now we can apply transform everywhere(test , validation or test dev set)
# - note that the training set values will be scaled to the specified range only if the new data contains outlierrs then they will be scaled out of range to tackle this,
#   just set the clip hyperparameter to True



housing_num



# There are two common ways to get all attributes to have the same scale: min
# max scaling and standardization :
# 1. MinMax scaler :
# - capping from 0 to 1.
# - feature_range hyperparameter to change  the range
# - NN prefers ero mean range (-1,1)
from sklearn.preprocessing import MinMaxScaler
min_max_scaler=MinMaxScaler(feature_range=(-1,1))
housing_num_min_max_scaled=min_max_scaler.fit_transform(housing_num)

# 2.Standard Scaler :
# - Unlike mean it doesnt restrict value to a specific range.
# - less affected by outliers.

from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()
housing_num_std_scaled=std_scaler.fit_transform(housing_num)





x = pd.DataFrame(housing_num_std_scaled, columns=housing_num.columns.tolist())
x


# dealing with heavy tail distributions :
reason : when the features distribution is heavy tailed if we use scalers then both the scalers will squash
         most values (around peak) around the zerom which all the models generally dont like.So it is prefered
         to turn the distribution in roughly symmetric distribution so that models can learn easily.
         The transformation converts the skewed distribution into  a bell shaped or gaussian 
         which is preffered by the models to elarn effectively

 ==== we should always transform before scaling the feature


1st approach :
- replacing the feature with its logarithmic, square root, or raise the power between 0 and 1.
- positive value wiht right skew == square root.
  moderate right skew == power tramsform (0<p<1)
  very heavy right tail == logarithmic  

2nd approach :
- using bucketing by percentiles 
- bucketing is breaking the continuous numeric features into discrete buckets.
- but bucketing here should be numeric like using percentiles :  bucket 0 - lowest 20 percentile,
  bucket 1 - 20 to 40 , ..... bucket 4 - top 20%
- after bucketing replace the original values with the bucket index.
- now the transformed feature is skew handled , doesnt care about outliers ,uniformly distributed ,moel ready no scaling needed.
- for example income_cat in satratified sampling.
- but bucket misses some details from the original data , works better when precise numeric relationshiips dont matter.



plt.hist(housing['housing_median_age'],bins=50)
plt.show()


# - As we can see the above graph has multiple clear peaks called modes this dsitributin is called
# multimodal distribution. Bucketizing these features can also be helpful but this time we bucketize
# them as categories not numbers so we dont say bucket 2 > bucket 1.

# - this means that after bucketizing them as categories we encoode them using onehotencoder when the
# number sof buckets is not huge.

# -This approach will allow the regression model to
#  more easily learn different rules for different ranges of this feature value. 
  
#   @Why This Works Better:
#  - Captures non-linear effects (like sudden drops or bumps in price)
#  - Allows the model to fit different patterns for each range
#  - One-hot encoding avoids incorrect assumptions about order/distance






